# 对于调参的操作及我个人的一些想法

## 本文基于`COCO128`数据集

### 早期
对于训练的早期处理，个人认为需要加上pretrain参数进行数据空间的确定，所以在早期的训练设置了10个epoch作为预训练轮，
其中有3个epoch是作为预训练的热身轮，以大bs，大lr作为快速梯度下降的基础，在这里我设置了workers=2，epoch=54，
优化器在第四轮epoch的时候介入，调低bs和lr，以更好降低梯度。

训练机卡的一，本来想截图放个loss的图，效果非常好，但是真的卡的一，edge寄了，怕不是散热不行

3.6 23：03 训练机又死机了，可能真的是PCIe速率有问题，这次没有数据保存下来，最后一次的log发现gpu0和1在未训练时就达到满载

### 中期
中期使用100轮epoch，hyperp使用了中等的速率，增大workers，将线程设为4，以降低bs大小，此阶段的bs将小于16，大于4，
lr = 0.01并根据优化器持续降低，此阶段的pretrain参数使用早期训练的best.pt

### 末期
后期使用1000轮epoch，加大epoch是因为希望patience机制介入，在即将过拟合的时候停止训练，达到梯度的整体最低，此时使用最严格的
hyperp，lr初始值就设为1e-4，lf最大为0.01，将模型向更加精细化的方向推进，bs最大设为4，优化器根据epoch完成度降低bs大小，
workers设为4
